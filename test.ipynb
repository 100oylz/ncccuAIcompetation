{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. 加载模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import load_all\n",
    "from Loss_Function import CLOULoss, DistributionFocalLoss\n",
    "from Model import Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.测试csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# image, csv = load_all(config.DIRPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# csv_slice = csv[3].iloc[:, 3:].values\n",
    "# csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. 定义Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "class_to_index = {\n",
    "    'E2': 0, 'B52': 1, 'B2': 2, 'Mirage2000': 3, 'F4': 4,\n",
    "    'F14': 5, 'Tornado': 6, 'J20': 7, 'JAS39': 8\n",
    "}\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    '''\n",
    "    将图片和[class, Bbox]转化为Tensor\n",
    "    返回值：\n",
    "    1. image: [bs, 640, 640, 3]\n",
    "    2. target: [bs, 物品个数, class, xmin, ymin, xmax, ymax]\n",
    "    '''\n",
    "    def __init__(self, path, transform):\n",
    "        super().__init__()\n",
    "        self.image, self.csv = load_all(path)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.image)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.transform(self.image[index])\n",
    "        csv = self.csv[index]\n",
    "        # filename width height class xmin ymin xmax ymax\n",
    "        target_array = csv.iloc[:, 3:].values\n",
    "        target = torch.zeros((20, 5))\n",
    "        for i in range(len(target_array)):\n",
    "            target[i, 0] = class_to_index[target_array[i][0]]\n",
    "            target[i, 1] = target_array[i][1]\n",
    "            target[i, 2] = target_array[i][2]\n",
    "            target[i, 3] = target_array[i][3]\n",
    "            target[i, 4] = target_array[i][4]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. 定义Dataloader, net, Loss_Function, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "dataset = MyDataset(config.DIRPATH, data_transform)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True)\n",
    "F1 = nn.BCELoss()\n",
    "F2 = CLOULoss()\n",
    "F3 = DistributionFocalLoss()\n",
    "reg_max, num_class = 20, 9\n",
    "net = Model(reg_max, num_class).to(device)\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.937)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5. 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 256, 3, 3], expected input[4, 768, 40, 40] to have 256 channels, but got 768 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\ProgramProject\\PycharmProject\\ObjectDetection_YOLO\\test.ipynb 单元格 11\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ProgramProject/PycharmProject/ObjectDetection_YOLO/test.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ProgramProject/PycharmProject/ObjectDetection_YOLO/test.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/ProgramProject/PycharmProject/ObjectDetection_YOLO/test.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m BLS1, CLS1, BLS2, CLS2, BLS3, CLS3 \u001b[39m=\u001b[39m net(image)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ProgramProject/PycharmProject/ObjectDetection_YOLO/test.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(BLS1\u001b[39m.\u001b[39mshape, BLS2\u001b[39m.\u001b[39mshape, BLS3\u001b[39m.\u001b[39mshape, CLS1\u001b[39m.\u001b[39mshape, CLS2\u001b[39m.\u001b[39mshape, CLS3\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ProgramProject/PycharmProject/ObjectDetection_YOLO/test.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m BLS \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((BLS1, BLS2, BLS3), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (b, 8400, 80)\u001b[39;00m\n",
      "File \u001b[1;32md:\\environment\\anaconda\\envs\\py38_AI\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32md:\\ProgramProject\\PycharmProject\\ObjectDetection_YOLO\\Model.py:15\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     14\u001b[0m     Backbone_out1, Backbone_out2, Backbone_out3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBackbone(x)\n\u001b[1;32m---> 15\u001b[0m     Neck_out1, Neck_out2, Neck_out3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mNeck(Backbone_out1, Backbone_out2, Backbone_out3)\n\u001b[0;32m     16\u001b[0m     BLS1, CLS1, BLS2, CLS2, BLS3, CLS3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mHead(Neck_out1, Neck_out2, Neck_out3)\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m BLS1, CLS1, BLS2, CLS2, BLS3, CLS3\n",
      "File \u001b[1;32md:\\environment\\anaconda\\envs\\py38_AI\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32md:\\ProgramProject\\PycharmProject\\ObjectDetection_YOLO\\netModule\\Neck.py:46\u001b[0m, in \u001b[0;36mNeck.forward\u001b[1;34m(self, x1, x2, x3)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39m# print(x3_upsample.shape)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x3_upsample \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((x2, x3_upsample), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m res\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2d(x3_upsample)\n\u001b[0;32m     47\u001b[0m \u001b[39m# print(x3_upsample.shape)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m# 第一个CSPLayer计算\u001b[39;00m\n\u001b[0;32m     49\u001b[0m x3_upsample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcsplayer_2conv_1(x3_upsample)\n",
      "File \u001b[1;32md:\\environment\\anaconda\\envs\\py38_AI\\lib\\site-packages\\torch\\nn\\modules\\module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[0;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[0;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[1;32md:\\environment\\anaconda\\envs\\py38_AI\\lib\\site-packages\\torch\\nn\\modules\\conv.py:399\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32md:\\environment\\anaconda\\envs\\py38_AI\\lib\\site-packages\\torch\\nn\\modules\\conv.py:395\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    392\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    393\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    394\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 395\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    396\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 256, 3, 3], expected input[4, 768, 40, 40] to have 256 channels, but got 768 channels instead"
     ]
    }
   ],
   "source": [
    "for i, (image, target) in enumerate(dataloader):\n",
    "    image = image.to(device)\n",
    "    target = target.to(device)\n",
    "    BLS1, CLS1, BLS2, CLS2, BLS3, CLS3 = net(image)\n",
    "    print(BLS1.shape, BLS2.shape, BLS3.shape, CLS1.shape, CLS2.shape, CLS3.shape)\n",
    "    BLS = torch.cat((BLS1, BLS2, BLS3), dim=1) # (b, 8400, 80)\n",
    "    CLS = torch.cat((CLS1, CLS2, CLS3), dim=1) # (b, 8400, 4)\n",
    "    print(BLS.shape, CLS.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
